############################ input configuration sections
[ct]
path_to_search = ../../data/resampled
filename_contains = ct
filename_not_contains = gtvt
spatial_window_size = (144, 144, 144)
interp_order = 2

[pt]
path_to_search = ../../data/resampled
filename_contains = pt
filename_not_contains = gtvt
spatial_window_size = (144, 144, 144)
interp_order = 2

[seg]
csv_file = ../../data/segmentation_output/inferred.csv
path_to_search = ../../data/segmentation_output
filename_contains = niftynet
spatial_window_size = (144, 144, 144)
interp_order = 2

[label]
path_to_search = ../../data/resampled
filename_contains = ct, gtvt
spatial_window_size = (144, 144, 144)
interp_order = 2

############################## system configuration sections
[SYSTEM]
cuda_devices = ""
num_threads = 1
num_gpus = 1
queue_length = 36
dataset_split_file = ../dataset_split.csv

[NETWORK]
#name = unet
name = dense_vnet
batch_size = 8
normalisation=False
whitening=False

# volume level preprocessing
volume_padding_size = 0
window_sampling = resize

[TRAINING]
sample_per_volume = 1
lr = 0.0003
loss_type = DicePlusXEnt
# CrossEntropy': 'niftynet.layer.loss_segmentation.cross_entropy
# CrossEntropy_Dense': 'niftynet.layer.loss_segmentation.cross_entropy_dense
# Dice': 'niftynet.layer.loss_segmentation.dice					Dice loss with square denominator
# DicePlusXEnt': 'niftynet.layer.loss_segmentation.dice_plus_xent_loss		(cross_entropy + Dice)
# Dice_Dense': 'niftynet.layer.loss_segmentation.dice_dense 			Computing mean-class Dice similarity. returns 1.0 - mean(Dice similarity per class)
# Dice_Dense_NS': 'niftynet.layer.loss_segmentation.dice_dense_nosquare		Computing mean-class Dice similarity with no square terms in the denominator
# Dice_NS': 'niftynet.layer.loss_segmentation.dice_nosquare			Function to calculate the classical dice loss
# GDSC': 'niftynet.layer.loss_segmentation.generalised_dice_loss : 		weighted dice loss for multiclass problems. Also works for 2 class problem
# SensSpec': 'niftynet.layer.loss_segmentation.sensitivity_specificity_loss	multiple-ground_truth version of the sensitivity-specificity loss
# Tversky': 'niftynet.layer.loss_segmentation.tversky				Tversky loss for imbalanced data. Dice = harmonic mean of precision and recall , i.e. FPs and FNs equally weighted. Tversky is generalization of the Dice similarity coefficient and the FÎ² scores
# VolEnforcement': 'niftynet.layer.loss_segmentation.volume_enforcement
# WGDL': 'niftynet.layer.loss_segmentation.generalised_wasserstein_dice_loss

save_every_n = 200
#validation_every_n = 50
#exclude_fraction_for_inference = 0.1
max_iter = 200

[INFERENCE]
border = (0, 0, 0)
inference_iter = -1
output_interp_order = 0
dataset_to_infer=inference

spatial_window_size = (144, 144, 144)
save_seg_dir = ../../../data/segmentation_output/

[EVALUATION]
save_csv_dir = ../../../data/evaluation_output
evaluations = dice,jaccard,false_positive_rate,positive_predictive_values,n_pos_ref,n_pos_seg
evaluation_units = foreground


############################ custom configuration sections
[SEGMENTATION]
image = pt,ct
inferred = seg
label = label
label_normalisation = True
output_prob = False
num_classes = 2
